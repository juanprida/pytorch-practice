{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1LBacVmqiuJmGgr1E6WKWLY43AKf1sJB-",
      "authorship_tag": "ABX9TyNBnJyygkAVDFgnc/Bk4pcl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanprida/pytorch-practice/blob/master/nn_toy_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "metadata": {
        "id": "_cwe5PRltylh"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4PY6AGHtbGl",
        "outputId": "4f064c20-53b4-490c-f855-dcf8401c995b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ],
      "metadata": {
        "id": "D-tXhknOtwAn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "vjCId-qjjhlg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_train_raw = pd.read_csv(\"/content/gdrive/MyDrive/Colab Notebooks/tabular_playground_series_sep_2022/train.csv\")\n",
        "pdf_test = pd.read_csv(\"/content/gdrive/MyDrive/Colab Notebooks/tabular_playground_series_sep_2022/test.csv\")"
      ],
      "metadata": {
        "id": "5-x6KzMewES1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_dateparts(\n",
        "    df: pd.DataFrame,\n",
        "    date: str\n",
        "    ) -> pd.DataFrame:\n",
        "    \"\"\"Add date parts to date column.\"\"\"\n",
        "\n",
        "    pdf = df.copy()\n",
        "    pdf[date] = pd.to_datetime(pdf[date])\n",
        "    pdf['year'] = pdf[date].dt.year\n",
        "    pdf['quarter'] = pdf[date].dt.quarter\n",
        "    pdf['month'] = pdf[date].dt.month\n",
        "    pdf['dayofweek'] = pdf[date].dt.dayofweek\n",
        "    pdf['dayofmonth'] = pdf[date].dt.day\n",
        "    pdf.drop(columns=[date], inplace=True)\n",
        "\n",
        "    return pdf\n",
        "\n",
        "def concat_frames(\n",
        "    pdf_train: pd.DataFrame,\n",
        "    pdf_test : pd.DataFrame, y: str\n",
        "    ) -> pd.DataFrame:\n",
        "    \"\"\" Concatenate train and test df.\"\"\"\n",
        "    \n",
        "    pdf_tr, pdf_te = pdf_train.copy(), pdf_test.copy()\n",
        "    pdf_tr['partition'], pdf_te['partition'] = 'train', 'test'\n",
        "    pdf_te[y] = np.NaN\n",
        "\n",
        "    return pd.concat([pdf_train, pdf_test])\n",
        "\n",
        "def preprocess(\n",
        "    settings_dict: dict,\n",
        "    pdf_fit: pd.DataFrame,\n",
        "    pdf_transform: pd.DataFrame\n",
        "    ) -> pd.DataFrame:\n",
        "    \"\"\"Encode columns based on settings_dict.\"\"\"\n",
        "\n",
        "    pdf_in = pdf_fit.copy()\n",
        "    pdf_out = pdf_transform.copy()\n",
        "\n",
        "    # Iterate through encoders.\n",
        "    for i in settings_dict:\n",
        "        # Get columns.\n",
        "        cols = settings_dict[i][0]\n",
        "        # Get sklearn encoder.\n",
        "        p = settings_dict[i][1]\n",
        "        # Fit encoder.\n",
        "        p.fit(pdf_in[cols])\n",
        "        # Drop columns and add encoded ones.\n",
        "        pdf_aux = pdf_out[cols]\n",
        "        pdf_out.drop(columns=cols, inplace=True)\n",
        "        pdf_out[p.get_feature_names_out()] = p.transform(pdf_aux[cols])\n",
        "    \n",
        "    return pdf_out\n",
        "\n",
        "def split_cols(\n",
        "    pdf: pd.DataFrame,\n",
        "    X_cols: list,\n",
        "    y_cols: list\n",
        "    ) -> pd.DataFrame:\n",
        "    \"\"\" Split pdf in X and y.\"\"\"\n",
        "\n",
        "    return pdf[X_cols].to_numpy(), pdf[y_cols].to_numpy()"
      ],
      "metadata": {
        "id": "9_2kqhQl0YrV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_train = add_dateparts(pdf_train_raw, 'date')\n",
        "\n",
        "preprocess_settings = {'categorical_columns': (['year', 'quarter', 'month', 'dayofweek', 'dayofmonth', 'country', 'store', 'product'], OneHotEncoder(handle_unknown='ignore', sparse=False))}\n",
        "pdf_train = preprocess(preprocess_settings, pdf_train, pdf_train)\n",
        "\n",
        "X_cols = [c for c in pdf_train if c not in ['row_id', 'num_sold']]\n",
        "y_cols = ['num_sold']\n",
        "\n",
        "X, y = split_cols(pdf_train, X_cols, y_cols)\n",
        "X_train, X_val, y_train, y_val  = train_test_split(X, y, random_state=97)"
      ],
      "metadata": {
        "id": "lv1RWAE1w_3s"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg = LinearRegression().fit(X_train, y_train)\n",
        "print(np.sum((reg.predict(X_val) - y_val) ** 2) / len(y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nh6gFAchkMPw",
        "outputId": "252fb9ca-dba2-4a65-d627-89cc541f03ec"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3013.3236938170203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-3\n",
        "lambda_l2 = 1e-5\n",
        "batch_size = 16"
      ],
      "metadata": {
        "id": "_0bMg_vizSgK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = TensorDataset(torch.Tensor(X_train).to(device), torch.Tensor(y_train).to(device))\n",
        "val_data = TensorDataset(torch.Tensor(X_val).to(device), torch.Tensor(y_val).to(device))\n",
        "\n",
        "train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
        "val_dataloader = DataLoader(val_data, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "zDCw96vjkE82"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class JuanNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(JuanNet, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_model = nn. Sequential(\n",
        "            nn.Linear(70, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, X):\n",
        "        X = self.flatten(X)\n",
        "        return self.linear_model(X)\n",
        "\n",
        "model = JuanNet().to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=lambda_l2)"
      ],
      "metadata": {
        "id": "rFIf_TgiseXX"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataloader, criterion, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    # To check what .train does.\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        pred = model(X)\n",
        "        loss = criterion(y, pred)\n",
        "\n",
        "        # Backprop.\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # loss, current = loss.item(), batch * len(X)\n",
        "        # print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def validate(model, dataloader, criterion):\n",
        "    size = len(dataloader.dataset)\n",
        "    n_batches = len(dataloader)\n",
        "    # To check what .eval does\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    out = {'pred': [], 'y': [], 'loss':[]} \n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            loss = criterion(y, pred)\n",
        "\n",
        "            out['pred'].append(pred)\n",
        "            out['y'].append(y)\n",
        "            out['loss'].append(loss)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "    print(test_loss / n_batches)\n",
        "    return out\n",
        "            # print(test_loss)"
      ],
      "metadata": {
        "id": "5LYoYs-D2x4L"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "for e in range(epochs):\n",
        "    print(f\"epoch {e}\")\n",
        "    train(model, train_dataloader, criterion, optimizer)\n",
        "print('Done')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Vhsz-zIwIdl",
        "outputId": "67007c38-93d4-46d2-f537-2443fc7b62ca"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0\n",
            "epoch 1\n",
            "epoch 2\n",
            "epoch 3\n",
            "epoch 4\n",
            "Done\n"
          ]
        }
      ]
    }
  ]
}